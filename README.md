# Attention Plasticity Toolkit

Utilities for measuring how much individual transformer attention heads rely on position and how flexibly they can change key preferences (“attention plasticity”). Feed it q/k dumps from [qk-sniffer](https://github.com/cgansen/qk-sniffer) (or any dataset matching the same schema) and it emits per-head CSV metrics plus visualizations.

## Highlights
- **Head-by-head analytics** – quantify positional predictability, residual normality, and overall attention plasticity for every `(layer, query head, key head)` triple.
- **Bucket-aware modeling** – automatically handles sliding-window dumps and only compares keys from earlier buckets when estimating plasticity.
- **Config-driven runs** – point the CLI at a YAML file to switch models, datasets, sampling budgets, worker count, and output destinations.
- **Publication-ready plots** – turn CSV outputs into layered trend plots with per-head traces and mean curves.

## Repository Map
```
attention_plasticity/    Core analysis + helpers (config, stats, plasticity, data utils)
analyze.py               CLI entry point for running the per-head analysis
scripts/plot_plasticity.py CLI tool to plot per-bucket plasticity curves
configs/                 Example run + plotting configs
results/                 Sample outputs (SmolLM2-135M)
tests/                   Pytest suite for the core functionality
```

## Requirements
- Python 3.10+
- A q/k dataset per attention head (typically generated by [qk-sniffer](https://github.com/viktor-shcherb/qk-sniffer)) published to Hugging Face Datasets or stored locally.
- System libs needed by `datasets`, `numpy`, `pandas`, `scipy`, and `matplotlib`.

Install dependencies in a fresh environment:
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

## Preparing Data
Each head needs two dataset configs: `lXXhYYq` (queries) and `lXXhZZk` (keys) stored under a base directory. Example (`configs/smollm2-135m.yaml`):
```
model_dir: HuggingFaceTB_SmolLM2_135M
num_layers: 30
num_q_heads: 9
num_k_heads: 3
```
The script infers key-head indices by grouping query heads evenly across key heads (supports GQA layouts). Datasets must expose a `train` split whose rows include
- `bucket` (int): geometric or uniform bucket id
- `position` (float): effective position in tokens
- `vector` (float list): embedding for that token
- Optional `sliding_window` column (int or null) indicating rolling-context window size; when set, attention plasticity sampling respects that window automatically (always including at least the immediately-previous bucket)

## Running the Analysis
1. Edit or copy `configs/smollm2-135m.yaml` to match your model, dataset name, bucket counts, and output paths.
2. Launch the CLI:
   ```bash
   python analyze.py --config configs/smollm2-135m.yaml
   ```
   Any flag can override the YAML at runtime (e.g., `--max_workers 4`, `--dataset_name your-user/sniffed-qk`).
3. The script processes every query head, writing
   - `head_metrics.csv`: per-head summary stats (positions R², MAE ratios, residual skew/kurtosis, KS pass rates, `ap_overall`).
   - `head_bucket_plasticity.csv`: plasticity per bucket per head (may contain NaNs if insufficient data).

Progress messages note which head has completed and highlight errors (e.g., shape mismatches, missing datasets).

## Plotting Plasticity
Turn the bucket CSV into a figure showing every head plus the mean curve:
```bash
python scripts/plot_plasticity.py --config configs/plots/smollm2-135m-plasticity.yaml
```
Key options (CLI flags or YAML):
- `bucket_csv`: path to `head_bucket_plasticity.csv`
- `bucket_type`: `uniform` (default) or `log` for geometric spacing
- `bucket_min_size`: bucket width (or multiplier for log scale)
- `model_name`: label added to the title
- `color_by_layer` or `color_trend`: highlight traces by layer index or monotonic trend
- `output`: save to a file instead of showing interactively

## Configuration Reference
Every run starts from a YAML file with at least these fields:
| Field | Description |
| --- | --- |
| `model_dir` | Base directory containing `lXXhYY{q,k}` dataset shards |
| `dataset_name` | Hugging Face dataset identifier (defaults to `viktoroo/sniffed-qk`) |
| `num_layers` | Transformer layers to scan |
| `num_q_heads` / `num_k_heads` | Query and key heads per layer; query heads must be a multiple of key heads |
| `max_tokens_per_head` | Cap on tokens sampled per head (random without replacement) |
| `normality_max_dims` | Max number of noise dims tested for KS statistics |
| `p_alpha` | Significance level for KS tests |
| `seed` | RNG seed controlling subsampling and bucket pair sampling |
| `output_csv`, `bucket_csv` | Paths for per-head and per-bucket outputs |
| `max_workers` | Process pool size (`None` = CPU count)

## Testing
Run the built-in tests to ensure dependencies and helpers are wired correctly:
```bash
pytest
```
Tests create dummy datasets to validate regressors, orientation logic, sliding-window handling, and config loading.

## Troubleshooting
- **Missing datasets**: confirm the Hugging Face dataset repo exists and contains the `data_dir` subfolders referenced via `model_dir`.
- **Shape mismatches**: verify query/key dumps use the same embedding width.
- **NaN plasticities**: occur when a bucket lacks either queries or at least two eligible keys; increase token caps or widen sliding windows.
- **Slow runs**: reduce `max_tokens_per_head` or `num_pairs_per_bucket` (edit `attention_plasticity/plasticity.py`), or set `--max_workers` to leverage more cores.

## Citation
If you use this toolkit in a paper or blog post, please credit “Attention Plasticity Toolkit (2025)” and link to this repository. Contributions, bug reports, and feature requests are welcome!
